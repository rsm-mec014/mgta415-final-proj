{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# import ssl\n",
        "\n",
        "# try:\n",
        "#     _create_unverified_https_context = ssl._create_unverified_context\n",
        "# except AttributeError:\n",
        "#     pass\n",
        "# else:\n",
        "#     ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hplnxkhuYNQC",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['text'] = data['review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "# Splitting the training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    data,  # Your feature vectors\n",
        "    data[\"label\"],  # The true labels\n",
        "    test_size=0.2,  # Specifies the proportion of data to include in the validation set\n",
        "    random_state=42  # Ensures reproducibility\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H6n9Vxh2YNQE"
      },
      "outputs": [],
      "source": [
        "# A function used to build a vocabulary based on descending word frequencies\n",
        "def build_vocab(sentences):\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    # Mapping from index to word\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return word_counts, vocabulary, vocabulary_inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5gV-kz2wYNQF"
      },
      "outputs": [],
      "source": [
        "def preprocess_df(df):\n",
        "    # get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.add('would')\n",
        "    # prepare translation table to translate punctuation to space\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    preprocessed_sentences = []\n",
        "    for i, row in df.iterrows():\n",
        "        sent = row[\"text\"]\n",
        "        sent_nopuncts = sent.translate(translator)\n",
        "        words_list = sent_nopuncts.strip().split()\n",
        "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
        "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
        "    df[\"text\"] = preprocessed_sentences\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[\"text\"] = X_train[\"review\"]\n",
        "X_val[\"text\"] = X_val[\"review\"]\n",
        "df_train = preprocess_df(X_train)\n",
        "df_test = preprocess_df(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_val_encoded = label_encoder.transform(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10515, 63), (10515,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10515, 63), (2629, 63))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.shape, df_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import PorterStemmer \n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "ps = PorterStemmer() \n",
        "\n",
        "# return a list of tokens\n",
        "def pre_processing_by_nltk(doc, stemming = True, need_sent = False):\n",
        "    # remove non-words\n",
        "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
        "    # get sentences\n",
        "    sentences = sent_tokenize(doc)\n",
        "    # get tokens\n",
        "    tokens = []\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        # step 3 (optional): stemming\n",
        "        if stemming:\n",
        "            words = [ps.stem(word) for word in words]\n",
        "        if need_sent:\n",
        "            tokens.append(words)\n",
        "        else:\n",
        "            tokens += words\n",
        "    return [w.lower() for w in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Initialize TfidfVectorizer with your custom tokenizer\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=True,\n",
        "                        preprocessor=None,  # Assuming preprocessing is already done\n",
        "                        tokenizer=pre_processing_by_nltk,  # Use your custom tokenizer\n",
        "                        use_idf=True,\n",
        "                        norm='l2',\n",
        "                        smooth_idf=True,\n",
        "                        min_df = 2,\n",
        "                        max_df = 0.98,\n",
        "                        ngram_range=(1, 3)\n",
        "                        )\n",
        "\n",
        "# Fit and transform the training data to create the training vectors\n",
        "train_vec = tfidf.fit_transform(df_train[\"text\"])\n",
        "test_vec = tfidf.transform(df_test[\"text\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10515, 515814)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10515,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf1 = LogisticRegression(max_iter=1000000000, \n",
        "                           random_state=42, \n",
        "                           multi_class= \"auto\",\n",
        "                            C = 10 ,\n",
        "                            warm_start= True)\n",
        "# Fit the model on the new training set\n",
        "clf1.fit(train_vec, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_preds = clf1.predict(test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-average F1 score: 0.7181248242859979\n",
            "Micro-average F1 score: 0.799923925446938\n",
            "Weighted-average F1 score: 0.7833277133508931\n"
          ]
        }
      ],
      "source": [
        "macro_f1 = f1_score(y_val_encoded, val_preds, average='macro')\n",
        "micro_f1 = f1_score(y_val_encoded, val_preds, average='micro')\n",
        "weighted_f1 = f1_score(y_val_encoded, val_preds, average='weighted')\n",
        "print(f'Macro-average F1 score: {macro_f1}')\n",
        "print(f'Micro-average F1 score: {micro_f1}')\n",
        "print(f'Weighted-average F1 score: {weighted_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7649296310384176\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(y_val_encoded, val_preds)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set a random seed\n",
        "random_seed = 42\n",
        "random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoding = tokenizer.batch_encode_plus(\n",
        "    X_train['text'],                    # List of input texts\n",
        "    padding=True,              # Pad to the maximum sequence length\n",
        "    truncation=True,           # Truncate to the maximum sequence length if necessary\n",
        "    return_tensors='pt',      # Return PyTorch tensors\n",
        "    add_special_tokens=True    # Add special tokens CLS and SEP\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input ID: tensor([[  101,  7842, 16475,  ...,     0,     0,     0],\n",
            "        [  101,  2057,  2253,  ...,     0,     0,     0],\n",
            "        [  101,  2204, 21122,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2026,  2564,  ...,     0,     0,     0],\n",
            "        [  101,  2023,  2173,  ...,     0,     0,     0],\n",
            "        [  101, 12090,  4840,  ...,     0,     0,     0]])\n",
            "Attention mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "input_ids = encoding['input_ids']  # Token IDs\n",
        "# print input IDs\n",
        "print(f\"Input ID: {input_ids}\")\n",
        "attention_mask = encoding['attention_mask']\n",
        "print(f\"Attention mask: {attention_mask}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10515, 512])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the batch size\n",
        "batch_size = 5 \n",
        "\n",
        "# Initialize an empty list to hold the embeddings\n",
        "word_embeddings = []\n",
        "\n",
        "# Process in batches\n",
        "for i in range(0, len(input_ids), batch_size):\n",
        "    # Get the batch\n",
        "    batch_input_ids = input_ids[i:i+batch_size]\n",
        "    batch_attention_mask = attention_mask[i:i+batch_size]\n",
        "    \n",
        "    # Perform the forward pass and get the embeddings\n",
        "    with torch.no_grad():\n",
        "        batch_outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        batch_word_embeddings = batch_outputs.last_hidden_state\n",
        "        word_embeddings.append(batch_word_embeddings)\n",
        "\n",
        "\n",
        "word_embeddings = torch.cat(word_embeddings, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the test data\n",
        "test_encoding = tokenizer.batch_encode_plus(\n",
        "    X_val['text'],                  # List of input texts from the test set\n",
        "    padding=True,                    # Pad to the maximum sequence length\n",
        "    truncation=True,                 # Truncate to the maximum sequence length if necessary\n",
        "    return_tensors='pt',             # Return PyTorch tensors\n",
        "    add_special_tokens=True          # Add special tokens CLS and SEP\n",
        ")\n",
        "\n",
        "test_input_ids = test_encoding['input_ids']\n",
        "test_attention_mask = test_encoding['attention_mask']\n",
        "\n",
        "# Generate embeddings for the test set\n",
        "test_word_embeddings = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_input_ids), batch_size):\n",
        "        batch_input_ids = test_input_ids[i:i+batch_size]\n",
        "        batch_attention_mask = test_attention_mask[i:i+batch_size]\n",
        "        batch_outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        batch_word_embeddings = batch_outputs.last_hidden_state\n",
        "        test_word_embeddings.append(batch_word_embeddings)\n",
        "\n",
        "# Concatenate all batches into one tensor for the test set\n",
        "test_word_embeddings = torch.cat(test_word_embeddings, dim=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
            "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
            "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from tensorflow) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from tensorflow) (4.10.0)\n",
            "Collecting wrapt>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.0.5-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from tensorflow) (1.26.2)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting rich (from keras>=3.0.0->tensorflow)\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
            "Collecting dm-tree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
            "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow)\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow)\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading tensorflow-2.16.1-cp311-cp311-macosx_12_0_arm64.whl (227.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.0/227.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.62.1-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.10.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-macosx_12_0_arm64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
            "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "Downloading dm_tree-0.1.8-cp311-cp311-macosx_11_0_arm64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, dm-tree, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
            "Successfully installed absl-py-2.1.0 astunparse-1.6.3 dm-tree-0.1.8 flatbuffers-24.3.7 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.0.5 libclang-16.0.6 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1 wheel-0.43.0 wrapt-1.16.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "! pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = len(set(y_train_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/wenpuzhang/Library/Python/3.11/lib/python/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "\n",
        "# Assuming X_train is your input features matrix with shape [n_samples, n_features]\n",
        "input_dim = 768  # For BERT embeddings, this would be 768. Adjust according to your feature size\n",
        "\n",
        "model = Sequential([\n",
        "    # First hidden layer\n",
        "    Dense(512, activation='relu', input_shape=(input_dim,)),  # Increased to 512 units\n",
        "    Dropout(0.5),\n",
        "    # Second hidden layer\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    # Third hidden layer\n",
        "    # Dense(256, activation='relu'),\n",
        "    # Dropout(0.5),\n",
        "    # Fourth hidden layer\n",
        "    # Dense(, activation='relu'),\n",
        "    # Dropout(0.5),\n",
        "    # Output layer\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adamax\n",
        "# Define your custom learning rate\n",
        "custom_learning_rate = 0.001\n",
        "# Initialize the Adamax optimizer with your custom learning rate\n",
        "optimizer = Adamax(learning_rate=custom_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2391 - loss: 2.1266 - val_accuracy: 0.5563 - val_loss: 1.5057\n",
            "Epoch 2/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4914 - loss: 1.5378 - val_accuracy: 0.6334 - val_loss: 1.1374\n",
            "Epoch 3/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5800 - loss: 1.2490 - val_accuracy: 0.6695 - val_loss: 0.9851\n",
            "Epoch 4/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6288 - loss: 1.1195 - val_accuracy: 0.6795 - val_loss: 0.9460\n",
            "Epoch 5/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6424 - loss: 1.0536 - val_accuracy: 0.6971 - val_loss: 0.9009\n",
            "Epoch 6/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6603 - loss: 1.0008 - val_accuracy: 0.6966 - val_loss: 0.8707\n",
            "Epoch 7/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6754 - loss: 0.9579 - val_accuracy: 0.7214 - val_loss: 0.8322\n",
            "Epoch 8/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6730 - loss: 0.9392 - val_accuracy: 0.7185 - val_loss: 0.8358\n",
            "Epoch 9/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6987 - loss: 0.9045 - val_accuracy: 0.7309 - val_loss: 0.8191\n",
            "Epoch 10/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7028 - loss: 0.9076 - val_accuracy: 0.7389 - val_loss: 0.8235\n",
            "Epoch 11/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7001 - loss: 0.8747 - val_accuracy: 0.7328 - val_loss: 0.7943\n",
            "Epoch 12/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7051 - loss: 0.8724 - val_accuracy: 0.7271 - val_loss: 0.8022\n",
            "Epoch 13/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7134 - loss: 0.8589 - val_accuracy: 0.7356 - val_loss: 0.7909\n",
            "Epoch 14/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7157 - loss: 0.8358 - val_accuracy: 0.7461 - val_loss: 0.7924\n",
            "Epoch 15/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7225 - loss: 0.8302 - val_accuracy: 0.7470 - val_loss: 0.7849\n",
            "Epoch 16/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7248 - loss: 0.8089 - val_accuracy: 0.7475 - val_loss: 0.7580\n",
            "Epoch 17/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7251 - loss: 0.8055 - val_accuracy: 0.7513 - val_loss: 0.7616\n",
            "Epoch 18/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7182 - loss: 0.8067 - val_accuracy: 0.7423 - val_loss: 0.7775\n",
            "Epoch 19/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7203 - loss: 0.8026 - val_accuracy: 0.7523 - val_loss: 0.7621\n",
            "Epoch 20/20\n",
            "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.7721 - val_accuracy: 0.7523 - val_loss: 0.7687\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_vec, y_train_encoded,\n",
        "                    batch_size=10,\n",
        "                    epochs=20,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907us/step\n",
            "Macro-average F1 score: 0.615314399931103\n",
            "Micro-average F1 score: 0.7402054012932675\n",
            "Weighted-average F1 score: 0.7135849007048611\n",
            "Accuracy: 0.7402054012932674\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict(test_vec)\n",
        "predictions = np.argmax(predictions, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Calculate F1 score\n",
        "macro_f1 = f1_score(y_val_encoded, predictions, average='macro')\n",
        "micro_f1 = f1_score(y_val_encoded, predictions, average='micro')\n",
        "weighted_f1 = f1_score(y_val_encoded, predictions, average='weighted')\n",
        "\n",
        "print(f'Macro-average F1 score: {macro_f1}')\n",
        "print(f'Micro-average F1 score: {micro_f1}')\n",
        "print(f'Weighted-average F1 score: {weighted_f1}')\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val_encoded, predictions)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOGISTIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average the embeddings across the sequence length for training set\n",
        "train_vec = torch.mean(word_embeddings, dim=1).cpu().numpy()\n",
        "\n",
        "# Average the embeddings across the sequence length for test set\n",
        "test_vec = torch.mean(test_word_embeddings, dim=1).cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10515, 768)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2629, 768)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf2 = LogisticRegression(max_iter=1000000000, \n",
        "                           random_state=42, \n",
        "                           multi_class= \"auto\",\n",
        "                            C = 10 ,\n",
        "                            warm_start= True)\n",
        "# Fit the model on the new training set\n",
        "clf2.fit(train_vec, y_train_encoded)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_preds = clf2.predict(test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro-average F1 score: 0.6910020482664272\n",
            "Micro-average F1 score: 0.7649296310384177\n",
            "Weighted-average F1 score: 0.7563900028673805\n"
          ]
        }
      ],
      "source": [
        "macro_f1 = f1_score(y_val_encoded, val_preds, average='macro')\n",
        "micro_f1 = f1_score(y_val_encoded, val_preds, average='micro')\n",
        "weighted_f1 = f1_score(y_val_encoded, val_preds, average='weighted')\n",
        "print(f'Macro-average F1 score: {macro_f1}')\n",
        "print(f'Micro-average F1 score: {micro_f1}')\n",
        "print(f'Weighted-average F1 score: {weighted_f1}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
